{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3970914a",
   "metadata": {},
   "source": [
    "# IO: Getting Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec9f983",
   "metadata": {},
   "source": [
    "3 classes are available to facilitate data access:\n",
    "\n",
    "- `HiveData`: Getting data from a Hive cluster, returning `Koalas` DataFrames\n",
    "- `PandasData`: Getting data from tables saved on disk, returning `Pandas` DataFrames\n",
    "- `PostgresData`: Getting data from a PostGreSQL DB, returning `Pandas` DataFrames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad31d102",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-08 14:54:26.449 | WARNING  | eds_scikit:<module>:31 - \n",
      "    To improve performances when using Spark and Koalas, please call `eds_scikit.improve_performances()`\n",
      "    This function optimally configures Spark. Use it as:\n",
      "    `spark, sc, sql = eds_scikit.improve_performances()`\n",
      "    The functions respectively returns a SparkSession, a SparkContext and an sql method\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "from eds_scikit.io import HiveData, PandasData, PostgresData"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4810071e",
   "metadata": {},
   "source": [
    "## Loading from Hive: `HiveData`\n",
    "\n",
    "The `HiveData` class expects two parameters:  \n",
    "\n",
    "- A `SparkSession` variable\n",
    "- The name of the Database to connect to\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83263eca",
   "metadata": {},
   "source": [
    "!!! aphp \"Using **Spark** kernels\"\n",
    "     All kernels designed to use Spark are configured to expose 3 variables at startup:  \n",
    "     \n",
    "     - `spark`, the current SparkSession\n",
    "     - `sc`, the current SparkContext\n",
    "     - `sql`, a function to execute SQL code on the Hive Database.  \n",
    "\n",
    "     In this case you can just provide the `spark` variable to `HiveData` !\n",
    "\n",
    "!!! tip \"Working with an I2B2 database\"\n",
    "     To use a built-in *I2B2 to OMOP* connector, specify `database_type=\"I2b2\"` when instantiating `HiveData`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b71d17",
   "metadata": {},
   "source": [
    "If needed, the following snippet allows to create the necessary variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c8ef85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "\n",
    "conf = SparkConf()\n",
    "sc = SparkContext(conf=conf)\n",
    "spark = SparkSession.builder \\\n",
    "                    .enableHiveSupport() \\\n",
    "                    .getOrCreate()\n",
    "sql = spark.sql"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c92862",
   "metadata": {},
   "source": [
    "The class `HiveData` provides a convenient interface to OMOP data stored in Hive.  \n",
    "The OMOP tables can be accessed as attribute and they are represented as [Koalas DataFrames](https://koalas.readthedocs.io/en/latest/getting_started/10min.html#10-minutes-to-Koalas).\n",
    "You simply need to mention your Hive database name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb2d9d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = HiveData(\n",
    "    \"cse_210038_20221219\",#DB_NAME,\n",
    "    spark,\n",
    "    database_type=\"I2B2\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ccca0df",
   "metadata": {},
   "source": [
    "By default, only a subset of tables are added as attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42780520",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['concept',\n",
       " 'visit_detail',\n",
       " 'note_deid',\n",
       " 'person',\n",
       " 'care_site',\n",
       " 'visit_occurrence',\n",
       " 'measurement',\n",
       " 'procedure_occurrence',\n",
       " 'condition_occurrence',\n",
       " 'fact_relationship',\n",
       " 'concept_relationship']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.available_tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608cdf81",
   "metadata": {},
   "source": [
    "`Koalas` DataFrames, like `Spark` DataFrames, rely on a *lazy* execution plan: As long as no data needs to be specifically collected, saved or displayed, no code is executed. It is simply saved for a later execution.  \n",
    "The main interest of Koalas DataFrames is that you can use (most of) the Pandas API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f95d3c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>birth_datetime</th>\n",
       "      <th>death_datetime</th>\n",
       "      <th>gender_source_value</th>\n",
       "      <th>cdm_source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1946-06-04</td>\n",
       "      <td>NaT</td>\n",
       "      <td>m</td>\n",
       "      <td>ORBIS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1940-01-21</td>\n",
       "      <td>2018-05-07</td>\n",
       "      <td>m</td>\n",
       "      <td>ORBIS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1979-04-25</td>\n",
       "      <td>NaT</td>\n",
       "      <td>m</td>\n",
       "      <td>ORBIS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2007-10-13</td>\n",
       "      <td>NaT</td>\n",
       "      <td>f</td>\n",
       "      <td>ORBIS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1964-12-27</td>\n",
       "      <td>NaT</td>\n",
       "      <td>f</td>\n",
       "      <td>ORBIS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  birth_datetime death_datetime gender_source_value cdm_source\n",
       "0     1946-06-04            NaT                   m      ORBIS\n",
       "1     1940-01-21     2018-05-07                   m      ORBIS\n",
       "2     1979-04-25            NaT                   m      ORBIS\n",
       "3     2007-10-13            NaT                   f      ORBIS\n",
       "4     1964-12-27            NaT                   f      ORBIS"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "person = data.person\n",
    "person.drop(columns = ['person_id']).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "124f6976",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "person['is_over_50'] = (person['birth_datetime'] >= datetime(1971,1,1))\n",
    "\n",
    "stats = (\n",
    "    person\n",
    "    .groupby('is_over_50')\n",
    "    .person_id\n",
    "    .count()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465b5c9b",
   "metadata": {},
   "source": [
    "Once data has been sufficiently aggregated, it can be converted back to Pandas, e.g. for plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dbfce661",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "is_over_50\n",
       "True     132794\n",
       "False     66808\n",
       "Name: person_id, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats_pd = stats.to_pandas()\n",
    "stats_pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef4977a",
   "metadata": {},
   "source": [
    "Similarily, if you want to work on the `Spark` DataFrame instead, a similar method is available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21fd5207",
   "metadata": {},
   "outputs": [],
   "source": [
    "person_spark = person.to_spark()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e519c27",
   "metadata": {},
   "source": [
    "## Persisting/Reading a sample to/from disk: `PandasData`\n",
    "\n",
    "Working with Pandas DataFrame is, when possible, more convenient.  \n",
    "You have the possibility to save your database or at least a subset of it.  \n",
    "Doing so allows you to work on it later without having to go through `Spark` again.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f4cbb0",
   "metadata": {},
   "source": [
    "!!! warning \"Careful with cohort size\"\n",
    "     Do not save it if your cohort is **big**: This saves **all** available tables on disk."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e552facc",
   "metadata": {},
   "source": [
    "For instance, let us define a dummy subset of 1000 patients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d60bed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "visits = data.visit_occurrence\n",
    "\n",
    "selected_visits = (\n",
    "    visits\n",
    "    .loc[visits[\"visit_source_value\"] == \"urgence\"]\n",
    ")\n",
    "\n",
    "sample_patients = (\n",
    "    selected_visits[\"person_id\"]\n",
    "    .drop_duplicates()\n",
    "    .head(1000)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684d4b0b",
   "metadata": {},
   "source": [
    "And save every table restricted to this small cohort as a `parquet` file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90471c76-def1-45c6-89d9-d3a7710ad91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "MY_FOLDER_PATH = \"./test_cohort\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97678c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "folder = os.path.abspath(MY_FOLDER_PATH)\n",
    "\n",
    "tables_to_save = [\"person\", \"visit_detail\", \"visit_occurrence\"]\n",
    "\n",
    "data.persist_tables_to_folder(\n",
    "    folder,\n",
    "    tables=tables_to_save,\n",
    "    person_ids=sample_patients\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ead263",
   "metadata": {},
   "source": [
    "Once you saved some data to disk, a dedicated class can be used to access it:  \n",
    "The class `PandasData` can be used to load OMOP data from a folder containing several parquet files. The tables\n",
    "are accessed as attributes and are returned as Pandas DataFrame.\n",
    "\n",
    "**Warning**: in this case, the whole table will be loaded into memory on a single jupyter server. Consequently it is advised\n",
    "to only use this for small datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "87b66bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = PandasData(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aeee7ebc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['visit_detail', 'visit_occurrence', 'person']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.available_tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5798a8ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type: <class 'pandas.core.frame.DataFrame'>\n",
      "shape: (1000, 5)\n"
     ]
    }
   ],
   "source": [
    "person = data.person\n",
    "print(f\"type: {type(person)}\")\n",
    "print(f\"shape: {person.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dba4d52",
   "metadata": {},
   "source": [
    "## Loading from PostGres: `PostgresData`\n",
    "\n",
    "OMOP data can be stored in a PostgreSQL database. The `PostgresData` class provides a convinient interface to it.\n",
    "\n",
    "**Note :** this class relies on the file `~/.pgpass` that contains your identifiers for several databases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8486d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = PostgresData(\n",
    "    dbname=DB, \n",
    "    schema=\"omop\", \n",
    "    user=USER,\n",
    ")\n",
    "\n",
    "data.read_sql(\"select count(*) from person\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scikit_thomas",
   "language": "python",
   "name": "scikit_thomas"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
